---
layout: post
title: "Bayesian Bandits: Thompson Sampling with Implementation and Regret Analysis"
date: 2022-09-01 05:32:20 +0300
description: Youâ€™ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. # Add post description (optional)
img: beta.jpg # Add image post (optional)
---

<strong>Thompson Sampling:</strong> At any round of the Thompson Sampling, we try to sample a bandit environment from the bandit space that is most likely to be close to the one we are dealing with right now, given the history of the previous rounds. So, we are in fact building a posterior distribution $Q_{t}(v | X_{t-1}, A_{n-1},\cdots, ,X_{1}, A_{1})$ where $v$ is the bandit environment and $A_{t}$ is the action taken at round $t$ and $X_{t}$ is the reward revealed after action $A_{t}$. For brevity, let's define $H_{t} = (X_{t}, A_{t},\cdots, ,X_{1}, A_{1})$ to be the history of exploration-exploitation and rewards. So, this approach returns a policy $\pi = (\pi_{t})_{t=1}^{\infty}$ so that,    
<center>$\pi_{t}(a|H_{t-1}) = Q_{t}(B_{a}|H_{t-1})$</center>
where, 
<center>$B_{a} = \{v \in \epsilon: \mu_{a}(v) = \arg \max_{b} \mu_{b}(v)\}$</center>
Ties are broken arbitrarily.

<strong>Pseudocode:</strong>      
<strong>for</strong> $t = 1, 2, \cdots , n$ <strong>do</strong>     
$\quad\quad$sample $v_{t}$ from $Q_{t}(.|H_{t-1})$     
$\quad\quad$choose $A_{t} = \arg \max_{1 \leq i \leq k} \mu_{i}(v_{t})$    
$\quad\quad$observe reward $X_{t}$     
$\quad\quad$update $H_{t} = (X_{t}, A_{t}) \oplus H_{t-1}$    
<strong>end for</strong>      

Here, $\oplus$ means concatenation.

<strong>A very easy example for Bernoulli Bandits:</strong> For every arm, we keep track of two variables $(\alpha, \beta)$. $\alpha$ means the number of rewards and $\beta$ is the number times you got no rewards. We use a <strong>Beta Distribution</strong> to sample a mean reward for this arm which is likely to be close to $\frac{\alpha}{\alpha + \beta}$. In this way, we get an estimate for the mean rewards of each arm. And this is the bandit $v_{t}$ which is mentioned in the pseudocode. We simply choose the best arm of $v_{t}$.    

Suppose, we are talking about ad recommendation. We have $k$ ads to render to a user. For each ad, we keep track of $(\alpha, \beta)$. An implementation of Thompson Sampling in this scenario is shown below:



<img src = "/assets/img/thompson1.jpg" height = "50%" width = "50%" style = "display: block; margin-left: auto; margin-right: auto;">
<img src = "/assets/img/thompson2.jpg" height = "50%" width = "50%" style = "display: block; margin-left: auto; margin-right: auto;">
<img src = "/assets/img/thompson4.jpg" height = "70%" width = "70%" style = "display: block; margin-left: auto; margin-right: auto;"> 


 <strong>Bayesian Regret Analysis:</strong> We will discuss the worst case Bayesian regret of Thompson Sampling for Bernoulli Bandits with 0-1 rewards and Gaussian bandits with 1-sub Gaussian arms.   
If the bandit has only 0-1 rewards, then for a particular arm, if it's played $T_{i}(t-1)$ times up to round $t$, then from, <strong>Hoeffding's Inequality</strong> we know that,    
<center>$Pr(|\bar u_{i}(t-1) - \mu_{i}| \geq \sqrt{\frac{1}{2 \max(1,T_{i}(t-1))}\log{\frac{2}{\delta}}} ) \leq \delta$</center>
If the bandit is 1-sub Gaussian, then we can say,    
<center>$Pr(|\bar u_{i}(t-1) - \mu_{i}| \geq \sqrt{\frac{2}{\max(1,T_{i}(t-1))}\log{\frac{1}{\delta}}} ) \leq \delta$</center>
So, in both cases, we can say that for all arms $i$ and for all rounds $t$,    
<center>$ |\bar u_{i}(t-1) - \mu_{i}| \leq \sqrt{\frac{1}{2 \max(1,T_{i}(t-1))}\log{\frac{2}{\delta}}}$</center>
holds for a good probability in the order of $1-\delta$ for each $i$ and $t$ with $\delta$ arbitrality close to 0. Such as, taking $\delta = \frac{1}{n^{2}}$.
Let's define this event to be the good event $G$. The bad event $G^{c}$ is then the event when any of the arms violate this property at any round.
So, <center>$Pr(G^{C}) = Pr(\cup_{i = 1, t = 1}^{i = k, t = n} {|\bar u_{i}(t-1) - \mu_{i}| > \sqrt{\frac{1}{2 \max(1,T_{i}(t-1))}\log{\frac{2}{\delta}}}}$</center>
Using the union bound,
<center>$Pr(G^{C}) \leq \sum_{i = 1}^{k} \sum_{t = 1}^{n} Pr(|\bar u_{i}(t-1) - \mu_{i}| > \sqrt{\frac{1}{2 \max(1,T_{i}(t-1))}\log{\frac{2}{\delta}}})$</center>

We know that, the Bayesian Regret can be expressed using the tower rule of conditional expectation with respect to the history $H_{t-1}$. So,    
<center>$R_{n} = \mathbb{E}[ \sum_{t=1}^{n} (\mu_{A^{*}} - \mu_{A_{t}}) ] =   \mathbb{E}[ \sum_{t=1}^{n} \mathbb{E}[\mu_{A^{*}} - \mu_{A_{t}}| H_{t-1}] ]$</center>


