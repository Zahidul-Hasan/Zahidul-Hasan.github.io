---
layout: post
title: "MDPs: Markov Chain, Markov Reward Model, Markov Decision Process"
date: 2022-10-11 05:32:20 +0300
description: Youâ€™ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. # Add post description (optional)
img: mdp.png# Add image post (optional)
---

<strong>Markov Chain:</strong> A Markov chain is a (possibly infinite) sequence of states where the transition from one state to another is stochastic. The set of all possible states could be countably infinite. In the Discrete-Time Markov Chain, the transition from one state to another is a discrete event. In the Continuous-Time Markov Chain, at any given moment the environment is changing from one state to another. Let's consider the following discrete example. For simplicity, we only assume that, there is only one type of weather possible for each day. Let's say, there are only three types of weather. And any day can be classified to only one of those. The following diagram shows the state transition probability for each weather type.
<img src = "/assets/img/mc.jpg" height = "70%" width = "70%">      
From this diagram, we can compute the probability of the following sequence of weather type: (sunny, sunny, windy, rainy, sunny) or in short (s, s, w, r, s). But how?    
<strong>Markov Assumption:</strong> A Markov Chain adheres to the Markov assumption that, the probably of the next state depends only on the current state. More formally,    
<center>$p(S_{n+1} = s_{n+1}| S_{n} = s_{n}, S_{n-1} = s_{n-1}, \cdots, S_{1} = s_{1}) = p(S_{n+1} = s_{n+1}| S_{n} = s_{n})$</center>    
Which can be succintly written as,
<center>$p(S_{n+1}| S_{n}, S_{n-1} , \cdots, S_{1}) = p(S_{n+1}| S_{n})$</center>        
So, using probability chain rule,
<center> $p(s, s, w, r, s) = p(s, w, r, s | S_{1} = s)p(S_{1} = s)$ </center>
<center> $= p(w, r, s | S_{2} = s, S_{1} = s) p(S_{2} = s | S_{1} = s) p(S_{1} = s)$ </center>
<center> $= p(r, s |S_{3} = w, S_{2} = s, S_{1} = s) p(S_{3} = w | S_{2} = s, S_{1} = s) p(S_{2} = s | S_{1} = s) p(S_{1} = s)$ </center>
<center> $= p(s |S_{4} = r, S_{3} = w, S_{2} = s, S_{1} = s) p(S_{4} = r | S_{3} = w, S_{2} = s, S_{1} = s) p(S_{3} = w | S_{2} = s, S_{1} = s) p(S_{2} = s | S_{1} = s) p(S_{1} = s)$ </center>       
Now, applying the markov assumption here, we get,     
<center> $p(s, s, w, r, s) = p(s |S_{4} = r) p(S_{4} = r | S_{3} = w) p(S_{3} = w | S_{2} = s) p(S_{2} = s | S_{1} = s) p(S_{1} = s)$ </center> 
<center> $p(s, s, w, r, s) = p(s | r) p(r | w) p(w | s) p(s | s) p(s)$ </center>        
Almost all of the quantities on the right can be immediately found from the above diagram. But how to find $p(S_{1} = s)$? This initial probability distribution among the starting states is problem dependent. Let's say, $v_{0}$ is an $n\times 1$ vector so that $v_{0,i}$ is the probability that state $i$ is the initial state.           
<strong>Steady State:</strong> If $A$ is the state transition probability matrix for a Markov Chain, then one can easily notice that $A^{n}[i][j]$ entry of $A^{n}$ gives us the probability of ending up at state $j$ from state $i$ via a random walk of length $n$. When $n$ is sufficiently large, we can expect this to mean the probability of ending up at state $j$ regardless of the starting state after a sufficientyly long random walk.  

