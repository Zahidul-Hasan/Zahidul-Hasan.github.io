---
layout: post
title: "MDPs: Markov Chain, Markov Reward Model, Markov Decision Process"
date: 2019-10-11 05:32:20 +0300
description: Youâ€™ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. # Add post description (optional)
img: Markov decision Process.png# Add image post (optional)
---

<strong>Markov Chain:</strong> A Markov chain is a (possibly infinite) sequence of states where the transition from one state to another is stochastic. The set of all possible states could be countably infinite. In the Discrete-Time Markov Chain, the transition from one state to another is a discrete event. In the Continuous-Time Markov Chain, at any given moment the environment is changing from one state to another. Let's consider the following discrete example. For simplicity, we only assume that, there is only one type of weather possible for each day. Let's say, there are only three types of weather. And any day can be classified to only one of those. The following diagram shows the state transition probability for each weather type.
<img src = "/assets/img/mc.jpg" height = "70%" width = "70%">
From this diagram, we can compute the probability of the following sequence of weather type: (sunny, sunny, windy, rainy, sunny). But how?    
<strong>Markov Assumption:</strong> A Markov Chain adheres to the Markov assumption that, the probably of the next state depends only on the current state. More formally,    
<center>$p(S_{n+1}| S_{n}, S_{n-1}, \cdots, S_{1}) = p(S_{n+1}| S_{n})$</center>


