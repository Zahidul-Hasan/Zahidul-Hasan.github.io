---
layout: post
title: "Contextual Bandits: A Literature Review"
date: 2022-05-10 13:32:20 +0300
description: Youâ€™ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. # Add post description (optional)
img: creature-keeper00005313still001-1654716362730.jpg # Add image post (optional)
---

Here in this post, I will provide excerpts from some of the most influential works done on Contextual Multi-Armed Bandits.     
<strong>Notation:</strong> $T$ is the horizon. $K$ is the number of arms. $\mathcal{M}$ is the set of policies.     
<strong> 2005: Wang et al, 2007: Pandey et al </strong>   
These works fall under the category of contextual bandits with side information and we will not elaborate on that.

<strong>2002: The Exp4 Algorithm by Auer et al </strong> The Exp4 algorithm can be perfectly used for the contextual bandit setting.
<ol>
  <li>
    <strong>Problem setting:</strong>
      <ul>
        <li>The bandit is adversarial.</li>
        <li>The environment is partially observable.</li>
    </ul>
  </li>
  <li>
    <strong>Regret Bounds:</strong>
    <ul>
      <li> The upper bound for the expected regret is $\mathcal{O}(\sqrt{TK\log{M}})$.</li>
    </ul>
  </li>
  <li>
    <strong>Drawbacks and Scope of Improvement:</strong>
    <ul>
      <li> The runtime is linear in $M$ in every round.</li>
      <li> The expected regret is quite good. But due to high variance, there is no guarantee that the regret stays concentrated around it's mean. </li>
    </ul>
  </li>
</ol>
<strong> 2007: The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information by Langford et al. </strong>
<ol>
  <li><strong>Problem Setting</strong>: 
    <ul>
      <li>The environment is partially observable. You can only observe the reward for the arm you have chosen.</li>
      <li>The bandit is stochastic.</li>
    </ul>
  </li>
  <li><strong>Key Properties</strong>: 
    <ul>
      <li> Exploration and Exploitation rounds are distinct from each other. In the exploration round, they pull an arm uniformly at random and use the arm's reward to learn about the bandit. But in the exploitation step, the reward collected from the best arm suggested by the algorithm is ignored.</li>
      <li> In the exploration step, their goal is not to maximize immediate reward but in exploitation step, it is. </li>
      <li> The reward of an unexplored arm in a round is estimated by an <strong>Inverse-Propensity-Score.</strong> </li>
      <li> Makes no assumption about the time horizon $T$. As, in the stochastic scenario, if $T$ is known before hands, then it is better to always perform all the exploration steps first and then do all the exploitation steps. Likewise, here they spread this idea of exploration first and then exploitation to multiple epoch. At the beginning of each epoch, they explore one arm at random. Then if the average regret per round in the previous rounds is $\epsilon$, then they perform ${\left\lceil \frac{1}{\epsilon}\right\rceil}$ exploitation steps per epoch.</li> 
      <li> Compares regret relative to the total reward of the best hypothesis. </li>
    </ul>
  </li>
  
  <li><strong>Regret Bound</strong>:
  <ul>
    <li> The worst case regret bound for epoch greedy is $\mathcal{O}(T^{\frac{2}{3}} (K \log{M})^{\frac{1}{3}})$ where $T$ is the time horizon, $K$ is the number of arms and $M$ is the size of the hypotheses class.</li>
  </ul>
  </li>
  <li><strong> Drawbacks and Scope of Improvement: </strong>
    <ul>
      <li> Compared to $Exp4$, has better regret bound dependence on $\log{M}$ and $K$ but worse dependence on $T$.</li>
    </ul>
  </li>
</ol>

<strong> 2009: NEXP Algorithm by McMahan et al. </strong>
<ol>
  <li>
    <strong>Problem Setting:</strong>
    <ul>
      <li>The bandit is non-stochastic.</li>
      <li>The environment is partially observable.</li>
    </ul>
  </li>
  <li>
    <strong>Key Properties:</strong>
    <ul>
      <li> Compares regret relative to the expected reward obtained by the best expert. </li>
      <li> Uses the fact that when $K$ is significantly large and even larger than $M$, then the regret upper bound need not be $\sqrt{TK \log{M}}$.</li>
    </ul>
  </li>
  <li><strong>Official Pseudocode:</strong>
    <ul>
   <li><img src="/assets/img/mcmahan.jpg" alt="pseudocode" width="45%" height="45%"></li>
      <li> <p>They have proposed several variants of the $F_{mix}$ subroutine. One of them is $UA_{mix}(\lambda)$:</p>         
        <center>$p(a) = (1-\lambda) \bar p(a) + \lambda \frac{1}{K}$</center>
        <p> where $\lambda$ is an egalitarianism factor </p>
      </li>
      <li> <p> Another variant of $F_{mix}$ is $UE_{mix}(\lambda)$:</p>
        <center>$p(a) = (1-\lambda) \bar p(a) + \frac{\lambda}{M} \sum_{i}^{} e_{i}(a)$</center>
      </li>
      <li> <p> Yet, another variant is called $LP_{mix}(\alpha)$:</p>
        <img src="/assets/img/lpmix.jpg" alt="lp subroutine" width="45%" height="45%">
      </li>
    </ul>
  </li>
  <li><strong>Regret Bounds:</strong>
    <ul>
      <li>
        When they use the $UA_{mix}$ subroutine, the expected regret is bounded from above by $E[R] \leq 2.63 \sqrt{TM \log{M}}$.
      </li>
      <li>
        When they use the $LP_{mix}$ subroutine, the expected regret is bounded from above by $E[R] \leq 2.63 \sqrt{TS \log{M}}$ where $S \leq min(K,M)$.
      </li>
    </ul>
  </li>
  <li>
    <strong>Drawbacks and Scope of Work:</strong>:
    <ul>
      <li> The runtime complexity in each round is $\mathcal{|A|+|M|}$.</li>
    </ul>
  </li>
</ol>
<strong>2011: The Exp4.p Algorithm by Beygelzimer et al </strong>
<ol>
  <li>
    <strong>Problem setting:</strong>
      <ul>
        <li>The bandit is adversarial.</li>
        <li>The environment is partially observable.</li>
        <li> Makes the assumption that $\log{M} \leq TK$.</li>
    </ul>
  </li>
  <li>
    <strong>Regret Bounds:</strong>
    <ul>
      <li> With probability at least $1-\delta$, expected regret is bounded by  $\mathcal{O}(\sqrt{TK\log{\frac{M}{\delta}}})$.</li>
      <li> With high probability, the algorithm guarantees that the upper bound for the expected regret is $\mathcal{O}(\sqrt{TK\log{M}})$.</li>
      <li> In the stochastic scenario, even if the policy class may be of an infinite size but if it has a finite VC dimension $d$, then the algorithm guarantees with high probability that the expected regret is bounded by $\mathcal{O}(\sqrt{Td\log{T}})$.</li>
    </ul>
  </li>
  <li>
    <strong>Drawbacks and Scope of Improvement:</strong>
    <ul>
      <li> Like it's predecessor $Exp4$, the runtime is linear in $M$ in every round.</li>
    </ul>
  </li>
</ol>
  <strong> 2014: ILOVETOCONBANDITS: Taming the Monster by Agarwal, Schapire et al </strong>
<ol>
  <li>
    <strong>Problem Setting:</strong>
    <ul>
      <li>The bandit is stochastic.</li>
      <li>We can only observe the reward for the action chosen.</li>
    </ul>
  </li>
  <li>
    <strong>Key Properties:</strong>
    <ul>
      <li>They use inverse propensity score to estimate the reward at round $t$ if they chose action $a$ given the history $H_{t-1} = (C_{1}, A_{1}, X_{1}, C_{2}, A_{2}, X_{2}, \cdots, C_{t-1}, A_{t-1}, X_{t-1})$.</li>
      <li>They have access to an Oracle that returns the policy that maximizes the total reward in the first $t-1$ rounds according to this estimated reward system.</li>
      <li> Compares regret relative to the expected reward obtained by the best policy.</li>
      <li> The algorithm calculates a distribution over the policy space. Then randomly samples a policy from that space. This policy also is a probability distribution over the actions. Then the algorithm chooses an action at random.</li>
      <li> Since they use the inverse propensity score to estimate the rewards, if the probability of an action being chosen at round $t$ is very low, then it induces a large variance on the reward estimate. So, they don't allow any of that probabilty to become less than $\mu \in [0, \frac{1}{K}]$.</li>
      <li> Unlike it's precursors such as Randomized UCB, this algorithm doesn't call the oracle in every round. It uses a epoch schedule like the doubling trick and calls the oracle only at the beginning of the epoch</li>
      <li> $\tau_{m}$ is the current epoch schedule and $\tau_{m+1}$ is the next epoch schedule, then $\tau_{m+1} \leq 2\tau_{m}$ </li>
      <li> Uses coordinate descent to solve the optimization problem </li>
      <li> Makes only $\sqrt{\frac{KT}{\log{\mathcal{O}}}}$ calls to the oracle which is phenomenal compared to it's precursor Randomized UCB </li>
    </ul>
  </li>
  <li><strong>Official Pseudocode:</strong>
    <ul>
   <li><img src="/assets/img/conbandit.jpg" alt="pseudocode" width="80%" height="80%"></li>
    </ul>
  </li>
  <li><strong>Regret Bounds:</strong>
    <ul>
      <li> Their algorithm gives optimal regret: $\mathcal{O}(\sqrt{TK\log{\frac{T\mathcal{M}}{\delta}}})$ with confidence at least $1-\delta$.</li> 
    </ul>
  </li>
  <li>
    <strong>Drawbacks and Scope of Work:</strong>:
    <ul>
      <li>Time complexity is $\sqrt{\frac{K^{3}T^{3}}{\log{\mathcal{O}}}}$. Can be optimized.</li>
      <li>The oracle is an offline regressor which prossesses batches of data at the start of each epoch. So, at the end of the end, the algorithm kind of uses stale knowledge about the environment.</li>
    </ul>
  </li>
</ol>
  <strong> 2014: Beyond UCB: SquareCB by Dylan Foster and Alexander Rakhlin</strong>
<ol>
  <li>
    <strong>Problem Setting:</strong>
    <ul>
      <li>The bandit is stochastic.</li>
      <li>We can only observe the reward for the action chosen.</li>
    </ul>
  </li>
  <li>
    <strong>Key Properties:</strong>
    <ul>
      <li>They use inverse propensity score to estimate the reward at round $t$ if they chose action $a$ given the history $H_{t-1} = (C_{1}, A_{1}, X_{1}, C_{2}, A_{2}, X_{2}, \cdots, C_{t-1}, A_{t-1}, X_{t-1})$.</li>
      <li>They have access to an Oracle that returns the policy that maximizes the total reward in the first $t-1$ rounds according to this estimated reward system.</li>
      <li> Compares regret relative to the expected reward obtained by the best policy.</li>
      <li> The algorithm calculates a distribution over the policy space. Then randomly samples a policy from that space. This policy also is a probability distribution over the actions. Then the algorithm chooses an action at random.</li>
      <li> Since they use the inverse propensity score to estimate the rewards, if the probability of an action being chosen at round $t$ is very low, then it induces a large variance on the reward estimate. So, they don't allow any of that probabilty to become less than $\mu \in [0, \frac{1}{K}]$.</li>
      <li> Unlike it's precursors such as Randomized UCB, this algorithm doesn't call the oracle in every round. It uses a epoch schedule like the doubling trick and calls the oracle only at the beginning of the epoch</li>
      <li> $\tau_{m}$ is the current epoch schedule and $\tau_{m+1}$ is the next epoch schedule, then $\tau_{m+1} \leq 2\tau_{m}$ </li>
      <li> Uses coordinate descent to solve the optimization problem </li>
      <li> Makes only $\sqrt{\frac{KT}{\log{\mathcal{O}}}}$ calls to the oracle which is phenomenal compared to it's precursor Randomized UCB </li>
    </ul>
  </li>
  <li><strong>Official Pseudocode:</strong>
    <ul>
   <li><img src="/assets/img/conbandit.jpg" alt="pseudocode" width="80%" height="80%"></li>
    </ul>
  </li>
  <li><strong>Regret Bounds:</strong>
    <ul>
      <li> Their algorithm gives optimal regret: $\mathcal{O}(\sqrt{TK\log{\frac{T\mathcal{M}}{\delta}}})$ with confidence at least $1-\delta$.</li> 
    </ul>
  </li>
  <li>
    <strong>Drawbacks and Scope of Work:</strong>:
    <ul>
      <li>Time complexity is $\sqrt{\frac{K^{3}T^{3}}{\log{\frac{\mathcal{M}}{\delta}}}}$. Can be optimized.</li>
      <li>The oracle is an offline regressor which prossesses batches of data at the start of each epoch. So, at the end of the end, the algorithm kind of uses stale knowledge about the environment.</li>
    </ul>
  </li>
</ol>
