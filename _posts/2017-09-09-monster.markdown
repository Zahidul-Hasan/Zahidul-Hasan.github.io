---
layout: post
title: "Contextual Bandits: A Literature Review"
date: 2022-05-10 13:32:20 +0300
description: Youâ€™ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. # Add post description (optional)
img: creature-keeper00005313still001-1654716362730.jpg # Add image post (optional)
---

Here in this post, I will provide excerpts from some of the most influential works done on Contextual Multi-Armed Bandits.      
<strong> 2005: Wang et al, 2007: Pandey et al </strong>   
These works fall under the category of contextual bandits with side information and we will not elaborate on that.

<strong> 2007: The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information by Langford et al. </strong>
<ol>
  <li><strong>Problem Setting</strong>: 
    <ul>
      <li>The environment is partially observable. You can only observe the reward for the arm you have chosen.</li>
      <li>The bandit is stochastic</li>
    </ul>
  </li>
  <li><strong>Key Properties</strong>: 
    <ul>
      <li> Exploration and Exploitation rounds are distinct from each other. In the exploration round, they pull an arm uniformly at random and use the arm's reward to learn about the bandit. But in the exploitation step, the reward collected from the best arm suggested by the algorithm is ignored.</li>
      <li> In the exploration step, their goal is not to maximize immediate reward but in exploitation step, it is. </li>
      <li> The reward of an unexplored arm in a round is estimated by an <strong>Inverse-Propensity-Score.</strong> </li>
      <li> Makes no assumption about the time horizon $T$. As, in the stochastic scenario, if $T$ is known before hands, then it is better to always perform all the exploration steps first and then do all the exploitation steps. Likewise, here they spread this idea of exploration first and then exploitation to multiple epoch. At the beginning of each epoch, they explore one arm at random. Then if the average regret per round in the previous rounds is $\epsilon$, then they perform ${\left\lceil \frac{1}{\epsilon}\right\rceil}$ exploitation steps per epoch.</li> 
      <li> Compares regret relative to the total reward of the best hypothesis. </li>
    </ul>
  </li>
  
  <li><strong>Regret Bound</strong>:
  <ul>
    <li> The worst case regret bound for epoch greedy is $\mathcal{O}(T^{\frac{2}{3}} (K \log{M})^{\frac{1}{3}})$ where $T$ is the time horizon, $K$ is the number of arms and $M$ is the size of the hypotheses class.</li>
  </ul>
  </li>
  <li><strong> Drawbacks and Scope of Improvement </strong>
    <ul>
      <li> Compared to $Exp4$, has better regret bound dependence on $\log{M}$ and $K$ but worse dependence on $T$.</li>
    </ul>
  </li>
</ol>

<strong> 2007: NEXP Algorithm by McMahan et al. </strong>
<ol>
  <li>
    <strong>Problem Setting:</strong>
    <ul>
      <li>The bandit is non-stochastic.</li>
      <li>The environment is partially observable.</li>
    </ul>
  </li>
  <li>
    <strong>Key Properties:</strong>
    <ul>
      <li> Compares regret relative to the expected reward obtained by the best expert. </li>
      <li> Uses the fact that when $K$ is significantly large and even larger than $M$, then the regret upper bound need not be $\sqrt{TK \log{M}}$.</li>
    </ul>
  </li>
  <li><strong>Official Pseudocode:</strong>
    <ul>
   <li><iframe src="https://drive.google.com/file/d/1D_E_NI61Nf3wazulFlMAxFChdAJi2cHR/preview" width="530" height="470" allow="autoplay"></iframe></li>
      <li> <p>They have proposed several variants of the $F_{mix}$ subroutine. One of them is $UA_{mix}(\lambda)$:</p>         
        <center>$p(a) = (1-\lambda) \bar p(a) + \lambda \frac{1}{K}$</center>
        <p> where $\lambda$ is an egalitarianism factor </p>
      </li>
      <li> <p> Another variant of $F_{mix}$ is $UE_{mix}(\lambda)$:</p>
        <center>$p(a) = (1-\lambda) \bar p(a) + \frac{\lambda}{M} \sum_{i}^{} e_{i}(a)$</center>
      </li>
      <li> <p> Yet, another variant is called $LP_{mix}(\alpha)$:</p>
        <iframe src="https://drive.google.com/file/d/1bAFLfdoX1zT5qoFCUujUKh7tX5ecKBoX/preview" width="580" height="300" allow="autoplay"></iframe>
      </li>
    </ul>
  </li>
  <li><strong>Regret Bounds:</strong>
    <ul>
      <li>
        When they use the $UA_{mix}$ subroutine, the expected regret is bounded from above by $E[R] \leq 2.63 \sqrt{TM \log{M}}$.
      </li>
      <li>
        When they use the $LP_{mix}$ subroutine, the expected regret is bounded from above by $E[R] \leq 2.63 \sqrt{TS \log{M}}$ where $S \leq min(K,M)$.
      </li>
    </ul>
  </li>
  <li>
    <strong>Drawback and Scope of Work</strong>:
    <ul>
      <li> The runtime complexity in each round is $\mathcal{K+M}$.</li>
    </ul>
  </li>
</ol>
