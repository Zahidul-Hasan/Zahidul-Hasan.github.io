---
layout: post
title: "Contextual Bandits: A Literature Review"
date: 2022-05-10 13:32:20 +0300
description: Youâ€™ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. # Add post description (optional)
img: creature-keeper00005313still001-1654716362730.jpg # Add image post (optional)
---

Here in this post, I will provide excerpts from some of the most influential works done on Contextual Multi-Armed Bandits.      
<strong> 2005: Wang et al, 2007: Pandey et al </strong>   
These works fall under the category of contextual bandits with side information and we will not elaborate on that.

<strong>2002: The Exp4 Algorithm by Auer et al </strong> The Exp4 algorithm can be perfectly used for the contextual bandit setting.
<ol>
  <li>
    <strong>Problem setting:</strong>
      <ul>
        <li>The bandit is adversarial.</li>
        <li>The environment is partially observable.</li>
    </ul>
  </li>
  <li>
    <strong>Regret Bounds:</strong>
    <ul>
      <li> The upper bound for the expected regret is $\mathcal{O}(\sqrt{TK\log{M}})$.</li>
    </ul>
  </li>
  <li>
    <strong>Drawbacks and Scope of Improvement:</strong>
    <ul>
      <li> The runtime is linear in $M$ in every round.</li>
      <li> The expected regret is quite good. But due to high variance, there is no guarantee that the regret stays concentrated around it's mean. </li>
    </ul>
  </li>
</ol>
<strong> 2007: The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information by Langford et al. </strong>
<ol>
  <li><strong>Problem Setting</strong>: 
    <ul>
      <li>The environment is partially observable. You can only observe the reward for the arm you have chosen.</li>
      <li>The bandit is stochastic.</li>
    </ul>
  </li>
  <li><strong>Key Properties</strong>: 
    <ul>
      <li> Exploration and Exploitation rounds are distinct from each other. In the exploration round, they pull an arm uniformly at random and use the arm's reward to learn about the bandit. But in the exploitation step, the reward collected from the best arm suggested by the algorithm is ignored.</li>
      <li> In the exploration step, their goal is not to maximize immediate reward but in exploitation step, it is. </li>
      <li> The reward of an unexplored arm in a round is estimated by an <strong>Inverse-Propensity-Score.</strong> </li>
      <li> Makes no assumption about the time horizon $T$. As, in the stochastic scenario, if $T$ is known before hands, then it is better to always perform all the exploration steps first and then do all the exploitation steps. Likewise, here they spread this idea of exploration first and then exploitation to multiple epoch. At the beginning of each epoch, they explore one arm at random. Then if the average regret per round in the previous rounds is $\epsilon$, then they perform ${\left\lceil \frac{1}{\epsilon}\right\rceil}$ exploitation steps per epoch.</li> 
      <li> Compares regret relative to the total reward of the best hypothesis. </li>
    </ul>
  </li>
  
  <li><strong>Regret Bound</strong>:
  <ul>
    <li> The worst case regret bound for epoch greedy is $\mathcal{O}(T^{\frac{2}{3}} (K \log{M})^{\frac{1}{3}})$ where $T$ is the time horizon, $K$ is the number of arms and $M$ is the size of the hypotheses class.</li>
  </ul>
  </li>
  <li><strong> Drawbacks and Scope of Improvement: </strong>
    <ul>
      <li> Compared to $Exp4$, has better regret bound dependence on $\log{M}$ and $K$ but worse dependence on $T$.</li>
    </ul>
  </li>
</ol>

<strong> 2007: NEXP Algorithm by McMahan et al. </strong>
<ol>
  <li>
    <strong>Problem Setting:</strong>
    <ul>
      <li>The bandit is non-stochastic.</li>
      <li>The environment is partially observable.</li>
    </ul>
  </li>
  <li>
    <strong>Key Properties:</strong>
    <ul>
      <li> Compares regret relative to the expected reward obtained by the best expert. </li>
      <li> Uses the fact that when $K$ is significantly large and even larger than $M$, then the regret upper bound need not be $\sqrt{TK \log{M}}$.</li>
    </ul>
  </li>
  <li><strong>Official Pseudocode:</strong>
    <ul>
   <li><img src="/assets/img/nex.jpg" alt="pseudocode" width="500" height="600"></li>
      <li> <p>They have proposed several variants of the $F_{mix}$ subroutine. One of them is $UA_{mix}(\lambda)$:</p>         
        <center>$p(a) = (1-\lambda) \bar p(a) + \lambda \frac{1}{K}$</center>
        <p> where $\lambda$ is an egalitarianism factor </p>
      </li>
      <li> <p> Another variant of $F_{mix}$ is $UE_{mix}(\lambda)$:</p>
        <center>$p(a) = (1-\lambda) \bar p(a) + \frac{\lambda}{M} \sum_{i}^{} e_{i}(a)$</center>
      </li>
      <li> <p> Yet, another variant is called $LP_{mix}(\alpha)$:</p>
        <img src="/assets/img/lpmix.jpg" alt="lp subroutine" width="500" height="300">
      </li>
    </ul>
  </li>
  <li><strong>Regret Bounds:</strong>
    <ul>
      <li>
        When they use the $UA_{mix}$ subroutine, the expected regret is bounded from above by $E[R] \leq 2.63 \sqrt{TM \log{M}}$.
      </li>
      <li>
        When they use the $LP_{mix}$ subroutine, the expected regret is bounded from above by $E[R] \leq 2.63 \sqrt{TS \log{M}}$ where $S \leq min(K,M)$.
      </li>
    </ul>
  </li>
  <li>
    <strong>Drawbacks and Scope of Work:</strong>:
    <ul>
      <li> The runtime complexity in each round is $\mathcal{|A|+|M|}$.</li>
    </ul>
  </li>
</ol>
<strong>2011: The Exp4.p Algorithm by Beygelzimer et al </strong>
<ol>
  <li>
    <strong>Problem setting:</strong>
      <ul>
        <li>The bandit is adversarial.</li>
        <li>The environment is partially observable.</li>
        <li> Makes the assumption that $\log{M} \leq TK$.</li>
    </ul>
  </li>
  <li>
    <strong>Regret Bounds:</strong>
    <ul>
      <li> With probability at least $1-\delta$, expected regret is bounded by  $\mathcal{O}(\sqrt{TK\log{\frac{M}{\delta}}})$.</li>
      <li> With high probability, the algorithm guarantees that the upper bound for the expected regret is $\mathcal{O}(\sqrt{TK\log{M}})$.</li>
      <li> In the stochastic scenario, even if the policy class may be of an infinite size but if it has a finite VC dimension $d$, then the algorithm guarantees with high probability that the expected regret is bounded by $\mathcal{O}(\sqrt{Td\log{T}})$.</li>
    </ul>
  </li>
  <li>
    <strong>Drawbacks and Scope of Improvement:</strong>
    <ul>
      <li> Like it's predecessor $Exp4$, the runtime is linear in $M$ in every round.</li>
    </ul>
  </li>
</ol>
  
