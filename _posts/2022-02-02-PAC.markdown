---
layout: post
title: "ML Theory: PAC Learnability and VC Dimensions"
date: 2022-04-06 13:32:20 +0300
description: Youâ€™ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. # Add post description (optional)
img: pac.jpg # Add image post (optional)
---

<strong>The Multi-Class Classification Problem</strong>: Let's say, we have a domain $\mathcal{X}$ where each element $x = (x_{1}, x_{2}, \cdots, x_{k})$ is a $k$ size vector of features, a distribution $\mathcal{D}$ over $\mathcal{X}$, and a label set $\mathcal{Y} = \\{1, 2, \cdots, n\\}$ so that there exists a function $c: \mathcal{X} \rightarrow \mathcal{Y}$. But we do not have any knowledge of the true distribution $\mathcal{D}$ and the function $c$. But we do have a finite sample $S = ((x_{1}, y_{1}), (x_{2}, y_{2}), \cdots, (x_{m}, y_{m}))$ where each pair is an input output mapping from $\mathcal{X} \times \mathcal{Y}$, i.e., $c(x_{i}) = y_{i}$ and some prior knowledge of the problem at hand.   
<br>
This prior knowledge of the problem is represented by a set of hypotheses called the <strong>Hypothesis Class</strong> $\mathcal{H}$. Every hypothesis $h \in \mathcal{H}$ is a conjectured solution to estimating the function $c$. The original function $c$ in this case, is called a concept. We are trying to learn the concept $c$ and $h$ is what we think of $c$. With the same domain and distribution, we could have a different concept $\bar c$ to learn and all of these different choices of $f$ creates the <strong>Concept Class</strong> we are trying to learn.        
<br>
<strong>Generalization Risk</strong>: Given a hpyothesis $h \in \mathcal{H}$, a target concept $f$ in concept class $F$, and a distribution $\mathcal{D}$ over domain $\mathcal{X}$, then the generalization risk or generalization error or true error of the hypothesis $h$ is defined to be,   
<center>$ R_{\mathcal{D}}(h) = Pr_{x \sim D} [h(x) \neq f(x)]$</center>    
<br>
<strong>Empirical Risk</strong>: Given a hpyothesis $h \in \mathcal{H}$, a target concept $f$ in concept class $F$, and a sample $S = ((x_{1}, y_{1}), (x_{2}, y_{2}), \cdots, (x_{m}, y_{m}))$, then the empirical risk of the hypothesis $h$ with respect to this sample $S$ is defined to be,
<center>$ R_{S}(h) = \frac{1}{m} \sum_{i = 1}^{m} \mathcal{I}\{h(x_{i}) \neq c(x_{i})\}$ </center>
The empirical risk is an unbiased estimator of the true risk.    
<br>
<strong>Empirical Risk Minimization Algorithm</strong>: Given a sample $S$ and a hypothesis class $\mathcal{H}$, a learning algorithm tries to come up with a hypothesis that is supposed to be least error-prone. But since it typically has no knowledge about the distribution $\mathcal{D}$, so, an available notion of error is the empirical risk with respect to the given sample. The algorithm tries to find a hypothesis that minimizes the empirical risk in sample $S$. Such an ERM algorithm could find a hypothesis that is specially designed to minimize the empirical risk in this sample but does very poor globally because the sample may not be a good representative of the whole domain and distribution.    
