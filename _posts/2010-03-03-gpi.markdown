---
layout: post
title: "RL: Generalized Policy Iteration: Dynamic Programming vs Monte Carlo"
date: 2022-10-05 05:32:20 +0300
description: Youâ€™ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. # Add post description (optional)
img: gppi.png # Add image post (optional)
---

The Major two dynamic programming approach to find an optimal policy given full knowledge of the MDP are Policy Iteration and Value Iteration. 
<br>
<strong>Policy Iteration:</strong> Start with an initial policy. Then apply Bellman equation repeatedly to determine state-values. From the state-values, greedily choose the action.
<hr>
<strong> POLICY ITERATION </strong>
<hr>
<strong>repeat until</strong> policy is stable:      
$\quad$ <strong>repeat until</strong> convergence:     
$\quad\quad$ <strong>for</strong> $s \in S$ <strong>do</strong>:      
$\quad\quad\quad v(s) = \sum_{r, s'} (r + \gamma v(s'))P(r,s'|s, \pi(s))$      
$\quad$ <strong>for</strong> $s \in S$ <strong>do</strong>:      
$\quad\quad \pi(s) = \arg \max_{a} \sum_{r, s'} (r + \gamma v(s'))P(r,s'|s, a)$
<br>
<strong>Value Iteration:</strong> Value iteration updates the state value of state $s$ from the optimal arm $a$. This way, the state values converge to optimal values. And the resulting policy too is optimal.
<hr>
<strong> POLICY ITERATION </strong>
<hr>
<strong>repeat until</strong> policy is stable:      
$\quad$ <strong>repeat until</strong> convergence:     
$\quad\quad$ <strong>for</strong> $s \in S$ <strong>do</strong>:      
$\quad\quad\quad v(s) = \max_{a} \sum_{r, s'} (r + \gamma v(s'))P(r,s'|s, a)$      
$<strong>for</strong> $s \in S$ <strong>do</strong>:      
$\quad \pi(s) = \arg \max_{a} \sum_{r, s'} (r + \gamma v(s'))P(r,s'|s, a)$
<br>

