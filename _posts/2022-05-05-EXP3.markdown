---
layout: post
title: "Adversarial Bandits: Hedge and Exp3"
date: 2022-05-05 13:32:20 +0300
description: Youâ€™ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. # Add post description (optional)
img: adv.gif # Add image post (optional)
---

In the case of stochastic bandits, our estimators such as the sample mean or the upper confidence bound gives a good glimpse of the fixed distributions that the arms are. But in the adversarial setting, we can run into problems if we choose naive algorithms such as $ETC$ or $UCB_{1}$, because in the case of ETC, after the exploration phase, we choose a possible best arm and stick to it. But in the adversial situation, right after we choose that particular arm, we could start getting all zeros. Or in the case of the UCB algorithms, every time we choose the best arm so far, we could get a zero which could in turn decrease it's confidence radius while at the same time reducing it's sample mean. Therefore, we need to re-assess our policies and introduce randomization and reformulate the notion of regret in the case of the adversarial bandits.  

Here we define the adversarial MAB scenario for today's analysis:
<ol>
  <li>The bandit has $k$ arms and there are $n$ rounds to the game. </li>
  <li>The bandit is <strong>deterministic</strong> and <strong>oblivious</strong> to the algorithm's choices, that is, $x \in [0,1]^{n\times k}$ is the reward table that is chosen beforehand.</li>
  <li>In the case of the Hedge algorithm, we will assume that the environment is fully observable. In the case of the Exp3 algorithm, we will assume that the algorithm is partially observable, i.e, the algorithm can only see the reward of the chosen arm.</li>
</ol>


<strong>The weak regret</strong>: It makes sense to define regret in this case in the following manner,  
$R_{n}(\pi, x) = \sum_{t = 1}^{n} ((max_{1 \leq i \leq k} x_{t,i})- x_{t,A_{t}})$    
Each time, comparing the algorithms choice with that of the best choice in round $t$, but this kind of regret is sort of mathematically intractable. So, we define a weaker version of this in the following way: The maximum reward is taken to be the maximum we could achieve if we stuck to one particular arm from the very beginning.    
$R_{n}(\pi, x) = max_{1 \leq i \leq k} (\sum_{t=1}^{n} x_{t,i} - x_{t,A_{t}})$   
This particular arm that maximizes the first summand is called the <strong>best arm in hindsight</strong>. The bandit literature for adversarial situations uses the word "loss" instead of "reward". So, instead of maximizing the reward, we will be minimizing the loss. And since, the maximum reward we can get is 1, we can define loss in the following way: $y_{t,i} = 1 - x_{t,i}$.    

There are various similar approaches to adversarial bandits starting with Weighted Majority Algorithm (WMA), Hedge, Exp3, Exp4 etc. Here will only discuss Hedge and Exp3. 
<strong>Hedge Algorithm</strong>: We start with $k$ experts for $k$ arms of the bandit, each having equal weight. In each round, according to this weighted probability distribution, we sample one of the arms for exploitation. Then all of the rewards in that round are revealed. From that we calculate the cost of each arm. We decrease the weights of the arms according to this cost. The costlier the arm, the more the decrease.    
$\textbf{Hedge}$:     
$\quad parameters: \epsilon \in (0,\frac{1}{2})$    
$\quad \textbf{Initialization}:$ for each arm $a$, set $W_{1}(a) = 1$   
$\quad$ for each round $t$:    
$\quad\quad$ let, $p_{t}(a) = \frac{w_{t}(a)}{\sum_{a = 1}^{k} w_{t}(a)}$     
$\quad\quad$ sample one arm $a_{t}$ from the given distribution $p_{t}$ and exploit it    
$\quad\quad$ observe the cost fore each arm      
$\quad\quad$ update the weight of each arm $a$ as: $w_{t+1}(a) = w_{t}(a) (1-\epsilon)^{cost_{t}(a)}$

This algorithm seems fairly intuitive. So, is Exp3 except the concept of <strong>Weighted estimators</strong> that are fake estimators of the reward in round $t$, if we chose arm $i$. And it is defined as follows:    
If our algorithm chooses arm $i$ in round $t$, then $\bar X_{t,i} = X_{t}/P_{t,i}$ where $P_{t,i}$ is the probability that having seen the action-reward series $A_{1}, X_{1}, \cdots, A_{t-1}, X_{t-1}$, the algorithm is going to choose arm $i$ in round $t$. Which means $p_{t,i} = P(A_{t} = i| A_{1}, X_{1}, \cdots, A_{t-1}, X_{t-1}$    
If we don't choose arm $i$ in round $t$, then  $\bar X_{t,i} = 0$.  
Notice that $E_{t-1}[\bar X_{t,i}] = x_{t,i}$ because, $E_{t-1}[X_{t,i}] = (\sum_{j \neq i} p_{t,j} 0 \bar X_{t,i}) + p_{t,i} \frac{x_{t,i}}{p_{t,i}}$. So, this $\bar X_{t,i}$ is an unbiased estimator of $x_{t,i}$.
Since, we are talking about losses instead of rewards, let's define $\bar Y_{t,i} = \mathbb{I}\{A_{t} = i\} \frac{y_{t,i}}{p_{t,i}}$, substituting $x_{t,i} = 1 - y_{t,i}$, we get, $\bar X_{t,i} = 1 - \mathbb{I}\{A_{t} = i\} \frac{1 - x_{t,i}}{p_{t,i}}$. 
